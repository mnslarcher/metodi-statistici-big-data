{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati utilizzati in questo notebook sono stati presi dalla competizione di Kaggle [Twitter Sentiment Analysis](https://www.kaggle.com/c/twitter-sentiment-analysis2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi del sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Twitter Sentiment Analysis](#twitter)<br>\n",
    "    1.1 [Descrizione](#descrizione)<br>\n",
    "2. [Analisi lessicale](#lessicale)<br>\n",
    "    2.1 [Sostituire pattern specifici](#sostituire)<br>\n",
    "    2.2 [Ridurre il tweet in *token*](#token)<br>\n",
    "    2.3 [Rimuovere le *stop word*](#stop_word)<br>\n",
    "    2.4 [Ridurre i *token* alla radice (*stemming*)](#stemming)<br>\n",
    "3. [Analisi esplorativa](#esplorativa)<br>\n",
    "    3.1 [Preparare i dati per l'analisi esplorativa](#preparare)<br>\n",
    "    3.2 [Visualizzare i *token* e gli *hashtag* più frequenti dividendo tra tweet positivi e negativi](#token_hashtag)<br>\n",
    "5. [Metriche di classificazione](#metriche)<br>\n",
    "5. [Classificare i tweet](#classificare)<br>\n",
    "    5.1 [Creare una baseline](#baseline)<br>\n",
    "    5.2 [Creare una pipeline di classificazione](#pipeline)<br>  \n",
    "6. [Analizzare la performance del modello](#performance)<br>\n",
    "7. [Analizzare il modello stimato](#analizzare_modello)<br>\n",
    "8. [Analizzare gli errori di previsione](#errori)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Twitter Sentiment Analysis](https://www.kaggle.com/c/twitter-sentiment-analysis2) <a id=twitter> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Descrizione <a id=descrizione> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This contest is taken from the real task of Text Processing.\n",
    "\n",
    "The task is to build a model that will determine the tone (neutral, positive, negative) of the text. To do this, you will need to train the model on the existing data (train.csv). The resulting model will have to determine the class (neutral, positive, negative) of new texts (test data that were not used to build the model) with maximum accuracy.\n",
    "\n",
    "> Nota: la descrizione parla di tre classi ma nel dataset sono presenti solo due classi. La metrica nella descrizione sembra essere l'accuratezza ma in Evaluation sembra invece essere l'F1 score. Noi consideriamo il problema come di classificazione binario e utilizzeremo come metrica principale l'F1 score.\n",
    "\n",
    "### Evaluation\n",
    "The evaluation metric for this competition is Mean F1-Score. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The F1 score is given by:\n",
    "$$\n",
    "F1 = 2\\frac{p \\cdot r}{p + r}\\, \\text{where}\\, p = \\frac{tp}{tp + fp},\\,  r = \\frac{tp}{tp + fn}\n",
    "$$\n",
    "The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leggere i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = \"datasets/twitter\"\n",
    "\n",
    "dati = pd.read_csv(PATH + \"/train.csv\", encoding=\"latin\")\n",
    "print(\"Dimensione del dataset: {} x {}\".format(*dati.shape))\n",
    "dati.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividere le variabili esplicative dalla variabile risposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dati[\"SentimentText\"].tolist(), dati[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Analisi lessicale <a id=lessicale> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sostituire pattern specifici <a id=sostituire> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sostituire i tag HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = X[91]\n",
    "print(\"Tweet:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo aver sostituito i tag HTML:\\n{}\".format(BeautifulSoup(tweet, \"lxml\").get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sostituire i collegamenti ipertestuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = X[16]\n",
    "print(\"Tweet:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo aver sostituito i collegamenti ipertestuali:\\n{}\".format(re.sub(\"http\\S+\", \" link \", tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ridurre il tweet in *token* <a id=token> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False, # se False: Questo è un ESEMPIO -> ['questo', 'è', 'un', 'esempio']\n",
    "    reduce_len=True, # se True: ma daiiiii non ci credooooo -> ['ma', 'daiii', 'non', 'ci', 'credooo']\n",
    "    strip_handles=True # se True: cosa ne pensi @mario? -> ['cosa', 'ne', 'pensi', '?']\n",
    ")\n",
    "\n",
    "tweet = X[2715]\n",
    "print(\"Tweet:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo la riduzione in token:\\n{}\".format(tokenizer.tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Rimuovere le *stop word* <a id=stop_word> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimuovere alcune *stop word* predefinite e la punteggiatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    "\n",
    "tweet = X[0]\n",
    "tweet = tokenizer.tokenize(tweet)\n",
    "print(\"Tweet dopo la riduzione in token:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo la rimozione delle stop words:\\n{}\".format([token for token in tweet if token not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimuovere i numeri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = X[3]\n",
    "tweet = tokenizer.tokenize(tweet)\n",
    "print(\"Tweet dopo la riduzione in token:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo la rimozione delle stop words:\\n{}\".format([token for token in tweet if not token.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ridurre i *token* alla radice (*stemming*) <a id=stemming> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tweet = X[1]\n",
    "tweet = tokenizer.tokenize(tweet)\n",
    "print(\"Tweet dopo la riduzione in token:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo la riduzione alla radice dei token:\\n{}\".format([stemmer.stem(token) for token in tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio\n",
    "\n",
    "1. Completare la funzione `tweet_analyzer()` definita in `msbd/preprocessamento/tweet_analyzer.py`. Sulla traccia di quanto visto finora, la funzione dovrà:\n",
    "   1. Sostituire i tag HTML e i collegamenti ipertestuali;\n",
    "   2. Trasformare il tweet in una lista di *token*;\n",
    "   3. Rimuovere le *stop word* (compresi i numeri come visto sopra);\n",
    "   5. Ridurre i *token* alla radice.\n",
    "2. Verificare la corettezza della funzione utilizzando pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msbd.preprocessamento import tweet_analyzer\n",
    "\n",
    "print(inspect.getsource(tweet_analyzer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -v msbd/tests/test_tweet_analyzer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esempio di tweet dopo il preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@student! analyze this &lt;3 tweeeet;, solution at http://www.fakelink.com :D 42 #42\"\n",
    "print(\"Tweet:\\n{}\".format(tweet))\n",
    "print(\"\\nTweet dopo la riduzione alla radice dei token:\\n{}\".format(tweet_analyzer(tweet, tokenizer, stemmer, stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analisi esplorativa <a id=esplorativa> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiere i dati in training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"# tweet in train: {} ({} pos / {} neg)\".format(len(X_train), (y_train == 1).sum(), (y_train == 0).sum()))\n",
    "print(\"# tweet in test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preparare i dati per l'analisi esplorativa <a id=preparare> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessare i tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preproc = [tweet_analyzer(tweet, tokenizer, stemmer, stop_words) for tweet in tqdm.tqdm(X_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creare le liste dei *token* appartenenti a tweet con sentimento postivo e negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pos = list(itertools.chain.from_iterable(list(itertools.compress(X_preproc, y_train == 1))))\n",
    "token_neg = list(itertools.chain.from_iterable(list(itertools.compress(X_preproc, y_train == 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creare le liste degli *hashtag* appartenenti a tweet con sentimento postivo e negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_pos = [token for token in token_pos if token.startswith(\"#\")]\n",
    "hashtag_neg = [token for token in token_neg if token.startswith(\"#\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Visualizzare i *token* e gli *hashtag* più frequenti dividendo tra tweet positivi e negativi <a id=token_hashtag> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creare un'istanza della classe `Counter` per ogni lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_token_pos = Counter(token_pos)\n",
    "c_token_neg = Counter(token_neg)\n",
    "c_hashtag_pos = Counter(hashtag_pos)\n",
    "c_hashtag_neg = Counter(hashtag_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafici a barre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"{} hashtag più frequenti nei tweet positivi\".format(N))\n",
    "plt.bar(*zip(*c_hashtag_pos.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"{} hashtag più frequenti nei tweet negativi\".format(N))\n",
    "plt.bar(*zip(*c_hashtag_neg.most_common(N)), color=\"midnightblue\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"{} token più frequenti nei tweet positivi\".format(N))\n",
    "plt.bar(*zip(*c_token_pos.most_common(N)), color=\"gold\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"{} token più frequenti nei tweet negativi\".format(N))\n",
    "plt.bar(*zip(*c_token_neg.most_common(N)), color=\"midnightblue\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuvole di parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = plt.imread(\"figures/twitter.jpg\")\n",
    "MAX_WORDS = 200\n",
    "MAX_FONT_SIZE = 200\n",
    "RELATIVE_SCALING = 1\n",
    "\n",
    "\n",
    "wc_pos = WordCloud(\n",
    "    mask=MASK,\n",
    "    max_words=MAX_WORDS, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    ").generate_from_frequencies(c_token_pos)\n",
    "\n",
    "wc_neg = WordCloud(\n",
    "    mask=MASK[:, ::-1, :],\n",
    "    max_words=MAX_WORDS,\n",
    "    background_color=\"midnightblue\",\n",
    "    max_font_size=MAX_FONT_SIZE,\n",
    "    relative_scaling=RELATIVE_SCALING,\n",
    "    colormap=plt.cm.YlOrRd\n",
    ").generate_from_frequencies(c_token_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(wc_pos, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(wc_neg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Metriche di classificazione <a id=metriche> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice di confusione e metriche derivabili da essa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![confusion_matrix](figures/confusion_matrix.png)\n",
    "\n",
    "*Immagine presa dalla pagina [Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix) di Wikipedia.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio\n",
    "\n",
    "1. Completare i metodi della classe `MetricheClassificazione` definita in `msbd/preprocessamento/metriche.py`;\n",
    "2. Verificare la corettezza dei metodi definiti utilizzando pytest.\n",
    "\n",
    "> Suggerimenti: \n",
    "> 1. Prendere ispirazione dai metodi già definiti;\n",
    "> 2. Eseguire il controllo con pytest ogni volta che si definisce un nuovo metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from msbd.metriche import MetricheClassificazione\n",
    "\n",
    "print(inspect.getsource(MetricheClassificazione))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest -v msbd/tests/test_metriche_classificazione.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "y_pred = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "print(\"# negativi: {}\".format(MetricheClassificazione.n_negativi(y_true, y_pred)))\n",
    "print(\"# positivi: {}\".format(MetricheClassificazione.n_positivi(y_true, y_pred)))\n",
    "print(\"# previsti negativi: {}\".format(MetricheClassificazione.n_previsti_negativi(y_true, y_pred)))\n",
    "print(\"# previsti positivi: {}\".format(MetricheClassificazione.n_previsti_positivi(y_true, y_pred)))\n",
    "print()\n",
    "print(\"Matrice di confusione:\")\n",
    "print(\"# veri negativi: {}\".format(MetricheClassificazione.n_veri_negativi(y_true, y_pred)))\n",
    "print(\"# falsi positivi: {}\".format(MetricheClassificazione.n_falsi_positivi(y_true, y_pred)))\n",
    "print(\"# falsi negativi: {}\".format(MetricheClassificazione.n_falsi_negativi(y_true, y_pred)))\n",
    "print(\"# veri positivi: {}\".format(MetricheClassificazione.n_veri_positivi(y_true, y_pred)))\n",
    "print()\n",
    "print(\"Tasso falsi positivi: {:.2f}\".format(MetricheClassificazione.tasso_falsi_positivi(y_true, y_pred)))\n",
    "print(\"Tasso veri positivi: {:.2f}\".format(MetricheClassificazione.tasso_veri_positivi(y_true, y_pred)))\n",
    "print(\"Precisione: {:.2f}\".format(MetricheClassificazione.precisione(y_true, y_pred)))\n",
    "print(\"Richiamo: {:.2f}\".format(MetricheClassificazione.richiamo(y_true, y_pred)))\n",
    "print(\"Punteggio F1: {:.2f}\".format(MetricheClassificazione.punteggio_f1(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classificare i tweet <a id=classificare> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Creare una baseline <a id=baseline> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msbd.grafici import grafico_matrice_confusione\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "dc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dc.predict(X_test)\n",
    "\n",
    "precisione_baseline = precision_score(y_test, y_pred)\n",
    "richiamo_baseline = recall_score(y_test, y_pred)\n",
    "f1_score_baseline = f1_score(y_test, y_pred)\n",
    "print(\"Precisione: {:.2f}\".format(precisione_baseline))\n",
    "print(\"Richiamo: {:.2f}\".format(richiamo_baseline))\n",
    "print(\"F1 score: {:.2f}\".format(f1_score_baseline))\n",
    "grafico_matrice_confusione(y_test, y_pred, [\"neg\", \"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio\n",
    "\n",
    "`DummyClassifier` ha un F1 score del 73% e un richiamo addirittura del 100%! Ѐ utile in un caso reale la previsione fatta da questo modello? Motivare la risposta e riflettere sul risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Creare una pipeline di classificazione <a id=pipeline> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definire la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(\n",
    "    analyzer=lambda t: tweet_analyzer(t, tokenizer, stemmer, stop_words),\n",
    "    min_df=50,\n",
    "    max_df=0.7,\n",
    ")\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=25)\n",
    "\n",
    "clf = Pipeline([('vect', vect), ('tree', tree)])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nota: tutti gli iperparametri sono stati scelti \"a priori\" e, sopratutto, senza prendere decisioni basate sull'insieme di *test*. Volendo scegliere la combinazione di iperparametri migliore tra un insieme di candidati (vedi *grid search*, *random search*, ...), avremmo bisogno anche di un terzo insieme di *validation*. Lo stesso vale per la scelta tra algoritmi diversi (es: `DecisionTreeClassifier`vs `LogisticRegression`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analizzare la performance del modello <a id=performance> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stimare, per ogni tweet del test set, la probabilità che il suo sentimento sia positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOGLIA_DECISIONALE = 0.5 # default\n",
    "\n",
    "y_score = clf.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_score > SOGLIA_DECISIONALE).astype(int) # equivalente a y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio\n",
    "\n",
    "Descrivere un caso in cui la soglia decisionale di default (0.5) non è adeguata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizzare la performance del modello fissata la soglia decisionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precisione: {:.2f} (baseline = {:.2f})\".format(precision_score(y_test, y_pred), precisione_baseline))\n",
    "print(\"Richiamo: {:.2f} (baseline = {:.2f})\".format(recall_score(y_test, y_pred), richiamo_baseline))\n",
    "print(\"F1 score: {:.2f} (baseline = {:.2f})\".format(f1_score(y_test, y_pred), f1_score_baseline))\n",
    "grafico_matrice_confusione(y_test, y_pred, [\"neg\", \"pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizzare le combinazioni di valori ottenibili per le metriche d'interesse al variare della soglia decisionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_25 = (y_score > 0.25).astype(int)\n",
    "y_pred_50 = (y_score > 0.5).astype(int)\n",
    "y_pred_75 = (y_score > 0.75).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msbd.grafici import grafico_curva_precisione_richiamo\n",
    "from msbd.grafici import grafico_curva_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER = \"*\"\n",
    "S = 100\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "grafico_curva_roc(y_test, y_score)\n",
    "plt.scatter(MetricheClassificazione.tasso_falsi_positivi(y_test, y_pred_75), recall_score(y_test, y_pred_75), \n",
    "            marker=MARKER, s=S, c=\"brown\", label=\"Soglia decisionale = 0.75\", zorder=3)\n",
    "plt.scatter(MetricheClassificazione.tasso_falsi_positivi(y_test, y_pred_50), recall_score(y_test, y_pred_50), \n",
    "            marker=MARKER, s=S, c=\"red\", label=\"Soglia decisionale = 0.5\", zorder=3)\n",
    "plt.scatter(MetricheClassificazione.tasso_falsi_positivi(y_test, y_pred_25), recall_score(y_test, y_pred_25), \n",
    "            marker=MARKER, s=S, c=\"tomato\", label=\"Soglia decisionale = 0.25\", zorder=3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "grafico_curva_precisione_richiamo(y_test, y_score)\n",
    "plt.scatter(recall_score(y_test, y_pred_75), precision_score(y_test, y_pred_75), \n",
    "            marker=MARKER, s=S, c=\"brown\", label=\"Soglia decisionale = 0.75\", zorder=3)\n",
    "plt.scatter(recall_score(y_test, y_pred_50), precision_score(y_test, y_pred_50), \n",
    "            marker=MARKER, s=S, c=\"red\", label=\"Soglia decisionale = 0.5\", zorder=3)\n",
    "plt.scatter(recall_score(y_test, y_pred_25), precision_score(y_test, y_pred_25), \n",
    "            marker=MARKER, s=S, c=\"tomato\", label=\"Soglia decisionale = 0.25\", zorder=3)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analizzare il modello stimato <a id=analizzare_modello> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzare l'albero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(\n",
    "    decision_tree=clf.named_steps[\"tree\"], \n",
    "    max_depth=4,\n",
    "    feature_names=clf.named_steps[\"vect\"].get_feature_names(),\n",
    "    class_names=(\"Neg\", \"Pos\"),\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    ")\n",
    "display(graphviz.Source(dot_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzare l'importanza delle variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msbd.grafici import grafico_importanza_variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM = 50\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "variabili = clf.named_steps[\"vect\"].get_feature_names()\n",
    "importanze = clf.named_steps[\"tree\"].feature_importances_\n",
    "\n",
    "titolo = \"Importanza delle prime {} variabili su {}\".format(MAX_NUM, len(variabili))\n",
    "\n",
    "grafico_importanza_variabili(importanze, variabili, max_num=MAX_NUM, titolo=titolo)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Analizzare gli errori di previsione <a id=errori> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preproc = [tweet_analyzer(tweet, tokenizer, stemmer, stop_words) for tweet in tqdm.tqdm(X_test)]\n",
    "tweet_score = pd.DataFrame({\"tweet\":X_test, \"tweet_preproc\": X_test_preproc, \"score\": y_score, \n",
    "                            \"sentimento\": y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vero sentimento negativo, previsto positivo con elevata confidenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "print(\"Primi {} tweet con sentimento negativo previsti con sentimento positivo:\".format(N))\n",
    "\n",
    "for _, riga in tweet_score[tweet_score[\"sentimento\"] == 0].sort_values(\"score\", ascending=False).head(N).iterrows():\n",
    "    print(\"\\nScore: {:.2f}\".format(riga[\"score\"]))\n",
    "    print(\"Tweet:\\n{}\".format(riga[\"tweet\"]))\n",
    "    print(\"Tweet dopo il preprocessamento:\\n{}\".format(riga[\"tweet_preproc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vero sentimento positivo, previsto negativo con elevata confidenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercizio\n",
    "\n",
    "Analizzare il caso in cui il vero sentimento era positivo ma il modello lo ha previsto negativo con elevata confidenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "print(\"Primi {} tweet con sentimento positivo previsti con sentimento negativo:\".format(N))\n",
    "\n",
    "# ============== YOUR CODE HERE ==============\n",
    "raise NotImplementedError\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
